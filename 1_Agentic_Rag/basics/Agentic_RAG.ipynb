{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "# load lora_paper.pdf documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./1_Agentic_Rag/basics/data/Lora.pdf\"]).load_data()\n",
    "# chunk_size of 1024 is a good default value\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Create nodes from documents\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# LLM model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Summary Index and Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "# summary index\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# vector store index\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Vector Indexes To Query Engines\n",
    "Once that we now have the vector indexes created and stored, we’ll now need to move ahead to creating the query engines that we’ll convert to tools aka query tools that our agents can use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary query engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Tools\n",
    "A query tool is simply a query engine with metadata, specifically a description of what the query tool can be used for or is for. This helps the router query engine to then be able to decide what query engine tool to route to depending on the query it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "query_engine=summary_query_engine,\n",
    "description=(\n",
    "    \"Useful for summarization questions related to the Lora paper.\"\n",
    "),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "query_engine=vector_query_engine,\n",
    "description=(\n",
    "    \"Useful for retrieving specific context from the the Lora paper.\"\n",
    "),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Query Engine\n",
    "Finally, we can go on ahead to creating the router query engine tool. This will enable us to use all the query tools we created from the query engines we defined above, specifically the summary_tool and the vector_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking for a summary of the document, which is typically related to summarization questions..\n",
      "\u001b[0mThe document introduces LoRA, a novel approach for adapting large language models to downstream tasks efficiently. LoRA involves freezing pre-trained model weights and incorporating trainable rank decomposition matrices into each layer of the Transformer architecture, reducing the number of trainable parameters. The method maintains high model quality without adding inference latency or limiting input sequence length. Empirical investigations demonstrate LoRA's effectiveness across tasks and models like RoBERTa, DeBERTa, GPT-2, and GPT-3. The study explores optimal rank selection for LoRA, subspace similarity between ranks, and the relationship between adaptation matrices and pre-trained weights. Overall, the document suggests that LoRA presents a competitive alternative to full fine-tuning, offering valuable insights into adapting pre-trained language models for downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question is asking for the long form of Lora, which is specific context related to the Lora paper..\n",
      "\u001b[0mThe long form of LoRA is Local Representation Adaptation.\n"
     ]
    }
   ],
   "source": [
    "#Let’s ask another question that does not involve the use of the summary tool.\n",
    "response = query_engine.query(\"What is the long from of Lora?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "Now that we have understood this basic pipeline, let’s move ahead into converting this into a pipeline function that we call utilize later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_router_query_engine(\n",
    "    document_fp: str,\n",
    "    verbose: bool = True,\n",
    ") -> RouterQueryEngine:\n",
    "    # load lora_paper.pdf documents\n",
    "    documents = SimpleDirectoryReader(input_files=[document_fp]).load_data()\n",
    "    \n",
    "    # chunk_size of 1024 is a good default value\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    # Create nodes from documents\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    \n",
    "    # LLM model\n",
    "    Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    # embedding model\n",
    "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "    \n",
    "    # summary index\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    # vector store index\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    \n",
    "    # summary query engine\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "\n",
    "    # vector query engine\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "    \n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=summary_query_engine,\n",
    "        description=(\n",
    "            \"Useful for summarization questions related to the Lora paper.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_query_engine,\n",
    "        description=(\n",
    "            \"Useful for retrieving specific context from the the Lora paper.\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    query_engine = RouterQueryEngine(\n",
    "        selector=LLMSingleSelector.from_defaults(),\n",
    "        query_engine_tools=[\n",
    "            summary_tool,\n",
    "            vector_tool,\n",
    "        ],\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking for a summary of the document, which is typically related to summarization questions..\n",
      "\u001b[0mThe document introduces a method called LoRA (Low-Rank Adaptation) that efficiently adapts large language models for specific tasks by incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. This method outperforms or matches traditional fine-tuning approaches on models like RoBERTa, DeBERTa, GPT-2, and GPT-3, while reducing GPU memory requirements and training throughput. The document also explores empirical investigations on rank-deficiency in language model adaptation, providing insights into low-rank matrices, model adaptation, subspace similarity in neural networks, correlation between layers, varying rank parameters, and task-specific directions in model adaptation. Experiments conducted on datasets like E2E NLG Challenge and MNLI under low-data regimes shed light on the performance and behavior of neural networks in various scenarios.\n"
     ]
    }
   ],
   "source": [
    "query_engine = await create_router_query_engine(\"./1_Agentic_Rag/basics/data/Lora.pdf\")\n",
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s move on ahead and create a utils.py file and have the following inside of it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/teamspace/studios/this_studio', '/home/zeus/miniconda3/envs/cloudspace/lib/python310.zip', '/home/zeus/miniconda3/envs/cloudspace/lib/python3.10', '/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/lib-dynload', '', '/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2560404057.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[38], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    from 1_Agentic_Rag.utils import create_router_query_engine\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd() \n",
    "# from utils import create_router_query_engine\n",
    "from 1_Agentic_Rag.utils import create_router_query_engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
