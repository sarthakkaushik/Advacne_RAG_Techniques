{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrence - https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/5/building-a-multi-document-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing get_doc_tools from Utils_FN_QE_tool.py\n",
    "# from Utils_FN_QE_tool import get_doc_tools\n",
    "\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
    "from typing import List, Optional\n",
    "\n",
    "def get_doc_tools(\n",
    "    file_path: str,\n",
    "    name: str,\n",
    ") -> str:\n",
    "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
    "\n",
    "    # load documents\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    \n",
    "    def vector_query(\n",
    "        query: str, \n",
    "        page_numbers: Optional[List[str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use to answer questions over the MetaGPT paper.\n",
    "    \n",
    "        Useful if you have specific questions over the MetaGPT paper.\n",
    "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
    "    \n",
    "        Args:\n",
    "            query (str): the string query to be embedded.\n",
    "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE \n",
    "                if we want to perform a vector search\n",
    "                over all pages. Otherwise, filter by the set of specified pages.\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        page_numbers = page_numbers or []\n",
    "        metadata_dicts = [\n",
    "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "        ]\n",
    "        \n",
    "        query_engine = vector_index.as_query_engine(\n",
    "            similarity_top_k=2,\n",
    "            filters=MetadataFilters.from_dicts(\n",
    "                metadata_dicts,\n",
    "                condition=FilterCondition.OR\n",
    "            )\n",
    "        )\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "        \n",
    "    \n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"vector_tool_{name}\",\n",
    "        fn=vector_query\n",
    "    )\n",
    "    \n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        name=f\"summary_tool_{name}\",\n",
    "        query_engine=summary_query_engine,\n",
    "        description=(\n",
    "            \"Use ONLY IF you want to get a holistic summary of MetaGPT. \"\n",
    "            \"Do NOT use if you have specific questions over MetaGPT.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return vector_query_tool, summary_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "papers = [\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/metagpt.pdf\",    \n",
    "    \"./2_ Multidoc_Agentic_RAG/data/longlora.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading all the papers and retruning for each paper their vectorindex and summary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/metagpt.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/longlora.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the experiments is the Redpajama dataset for training and the PG19 dataset for evaluation. Additionally, the cleaned Arxiv Math proof-pile dataset was also used for evaluation purposes.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results indicate that the LongLoRA method effectively extends the context length of large language models while maintaining minimal accuracy compromise. It shows improved perplexity with longer context sizes, demonstrating its effectiveness. The method can fine-tune models to significantly larger context lengths, such as extending Llama2 7B to 100k context length and 70B model to 32k context length on a single 8 Ã— A100 machine. Furthermore, the method showcases promising results in passkey retrieval accuracy and topic retrieval tasks, highlighting its versatility and performance across various evaluation scenarios.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA includes the Redpajama dataset for training, the PG19 dataset for evaluation, and the cleaned Arxiv Math proof-pile dataset for evaluation purposes.\n",
      "\n",
      "The evaluation results show that the LongLoRA method effectively extends the context length of large language models while maintaining minimal accuracy compromise. It demonstrates improved perplexity with longer context sizes, showcasing its effectiveness. The method can fine-tune models to significantly larger context lengths, such as extending Llama2 7B to 100k context length and 70B model to 32k context length on a single 8 x A100 machine. Additionally, the method performs well in passkey retrieval accuracy and topic retrieval tasks, indicating its versatility and performance across various evaluation scenarios.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework designed to improve the quality and accuracy of large language models (LLMs) by incorporating retrieval mechanisms and self-assessment. It involves training a Critic LM and a Generator LM to evaluate text using reflection tokens, ensuring that the generated responses are factually grounded and aligned with the predicted reflection tokens. This method outperforms traditional LLMs with more parameters and conventional retrieval-augmented generation approaches, as it enables the LM to adapt its behavior to various task requirements and produce informative and supported text outputs.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of pre-trained large language models, such as Llama2 models, with minimal accuracy compromise. It introduces Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training. This method reduces GPU memory cost and training time compared to standard full fine-tuning, while retaining the original attention architecture during inference. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers, enabling the extension of Llama2 models to significantly larger context lengths efficiently.\n",
      "=== LLM Response ===\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework designed to enhance the quality and accuracy of large language models (LLMs) by incorporating retrieval mechanisms and self-assessment. It involves training a Critic LM and a Generator LM to evaluate text using reflection tokens, ensuring that the generated responses are factually grounded and aligned with the predicted reflection tokens. This method outperforms traditional LLMs with more parameters and conventional retrieval-augmented generation approaches, as it enables the LM to adapt its behavior to various task requirements and produce informative and supported text outputs.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of pre-trained large language models, such as Llama2 models, with minimal accuracy compromise. It introduces Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training. This method reduces GPU memory cost and training time compared to standard full fine-tuning, while retaining the original attention architecture during inference. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers, enabling the extension of Llama2 models to significantly larger context lengths efficiently.\n",
      "assistant: Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework designed to enhance the quality and accuracy of large language models (LLMs) by incorporating retrieval mechanisms and self-assessment. It involves training a Critic LM and a Generator LM to evaluate text using reflection tokens, ensuring that the generated responses are factually grounded and aligned with the predicted reflection tokens. This method outperforms traditional LLMs with more parameters and conventional retrieval-augmented generation approaches, as it enables the LM to adapt its behavior to various task requirements and produce informative and supported text outputs.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of pre-trained large language models, such as Llama2 models, with minimal accuracy compromise. It introduces Shifted Sparse Attention (S2-Attn) to approximate the standard self-attention pattern during training. This method reduces GPU memory cost and training time compared to standard full fine-tuning, while retaining the original attention architecture during inference. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers, enabling the extension of Llama2 models to significantly larger context lengths efficiently.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/metagpt.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/longlora.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/loftq.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/swebench.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/selfrag.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/zipformer.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/values.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/finetune_fair_diffusion.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/knowledge_card.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/metra.pdf\",\n",
    "    \"./2_ Multidoc_Agentic_RAG/data/vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/metagpt.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/longlora.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/loftq.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/swebench.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/selfrag.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/zipformer.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/values.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/knowledge_card.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/metra.pdf\n",
      "Getting tools for paper: ./2_ Multidoc_Agentic_RAG/data/vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "len(all_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the problem with many large document is as the no of dcoument increase your llm context window may not be able to fit the name of the tools , hence may lead to wrong selection of tools.\n",
    "Solution- We do RAG for tool selection and then pass the relevant tool for calling to the llm\n",
    "- We need to index the toll -since these tools are python object we need to serialze them and convert them into string representation- this is done using \"Object index\" in llamaindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of MetaGPT. Do NOT use if you have specific questions over MetaGPT.', name='summary_tool_values', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of MetaGPT. Do NOT use if you have specific questions over MetaGPT.', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of MetaGPT. Do NOT use if you have specific questions over MetaGPT.', name='summary_tool_metra', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metra with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench is constructed by collecting pull requests from the top 100 most downloaded PyPI libraries in August 2023. It consists of task instances derived from merged pull requests that resolve issues in the repositories and introduce new tests. Each task instance includes information about the codebase, problem statements, test patches, and gold patches. The dataset is continuously updated with new task instances based on PRs created after the training date of language models. The validation process ensures the usability and correctness of each task instance, making it a realistic and challenging environment for evaluating language models.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench is constructed by collecting pull requests from the top 100 most downloaded PyPI libraries in August 2023. It consists of task instances derived from merged pull requests that resolve issues in the repositories and introduce new tests. Each task instance includes information about the codebase, problem statements, test patches, and gold patches. The dataset is continuously updated with new task instances based on PRs created after the training date of language models. The validation process ensures the usability and correctness of each task instance, making it a realistic and challenging environment for evaluating language models.\n",
      "assistant: The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench is constructed by collecting pull requests from the top 100 most downloaded PyPI libraries in August 2023. It consists of task instances derived from merged pull requests that resolve issues in the repositories and introduce new tests. Each task instance includes information about the codebase, problem statements, test patches, and gold patches. The dataset is continuously updated with new task instances based on PRs created after the training date of language models. The validation process ensures the usability and correctness of each task instance, making it a realistic and challenging environment for evaluating language models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA paper\"}\n",
      "=== Function Output ===\n",
      "The LongLoRA paper presents an efficient fine-tuning approach for extending the context lengths of large language models by combining improved LoRA with shifted sparse attention (S2-Attn). It introduces an Action Units Relation Learning framework that includes the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) components. The ART encoder models relations between different facial action units at AU-agnostic patches to aid in forgery detection, while TAP tampers with AU-related regions to provide Local Tampering Supervision. The paper achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, offers qualitative visualizations of tampered regions, and proposes an effective framework for deepfake detection.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ paper\"}\n",
      "=== Function Output ===\n",
      "LoftQ is a novel quantization framework introduced in a paper that focuses on applying quantization and Low-Rank Adaptation (LoRA) fine-tuning to pre-trained models. It aims to simultaneously quantize a Large Language Model (LLM) and find a proper low-rank initialization for LoRA fine-tuning. LoftQ significantly improves generalization in downstream tasks and outperforms existing quantization methods, especially in challenging low-bit scenarios. The paper evaluates LoftQ on various tasks such as natural language understanding, question answering, summarization, and natural language generation, showcasing its effectiveness and robustness. Additionally, LoftQ is implemented based on the publicly available Huggingface code-base and involves specific hyperparameters tailored for different tasks and quantization methods. The paper also extends LoftQ to convolutional layers, demonstrating its applicability beyond just natural language understanding tasks and comparing it favorably with pruning methods in reducing memory usage during training and storage phases.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning approach for extending the context lengths of large language models by combining improved LoRA with shifted sparse attention (S2-Attn). It presents an Action Units Relation Learning framework that includes the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) components. The ART encoder models relations between different facial action units at AU-agnostic patches to aid in forgery detection, while TAP tampers with AU-related regions to provide Local Tampering Supervision. The paper achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, offers qualitative visualizations of tampered regions, and proposes an effective framework for deepfake detection.\n",
      "\n",
      "On the other hand, LoftQ is a novel quantization framework that focuses on applying quantization and Low-Rank Adaptation (LoRA) fine-tuning to pre-trained models. LoftQ aims to simultaneously quantize a Large Language Model (LLM) and find a proper low-rank initialization for LoRA fine-tuning. It significantly improves generalization in downstream tasks and outperforms existing quantization methods, especially in challenging low-bit scenarios. LoftQ is evaluated on various tasks such as natural language understanding, question answering, summarization, and natural language generation, demonstrating its effectiveness and robustness. Additionally, LoftQ is implemented based on the publicly available Huggingface code-base and involves specific hyperparameters tailored for different tasks and quantization methods. The paper also extends LoftQ to convolutional layers, showcasing its applicability beyond just natural language understanding tasks and comparing it favorably with pruning methods in reducing memory usage during training and storage phases.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
